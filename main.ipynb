{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ISIC 2018 Skin Lesion Classification - EfficientNet B1\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "5d60a09f",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install -r requirements.txt\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "35370077",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "CONFIGURATION\n",
                        "============================================================\n",
                        "Device: cpu\n",
                        "Random Seed: 42\n",
                        "\n",
                        "Model: efficientnet_b1\n",
                        "Input Size: 224x224\n",
                        "Number of Classes: 7\n",
                        "\n",
                        "Batch Size: 16\n",
                        "Learning Rate: 0.0001\n",
                        "Weight Decay: 0.0001\n",
                        "Number of Epochs: 10\n",
                        "Early Stop Patience: 2\n",
                        "\n",
                        "Model Path: outputs\\models\\efficientnet_b1_isic2018.pt\n",
                        "============================================================\n",
                        "\n",
                        "‚úì All modules imported successfully!\n",
                        "PyTorch version: 2.9.1+cpu\n",
                        "CUDA available: False\n"
                    ]
                }
            ],
            "source": [
                "# Cell 1: Import c√°c module v√† c·∫•u h√¨nh\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import sys\n",
                "from tqdm.notebook import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Import custom modules t·ª´ th∆∞ m·ª•c scr\n",
                "from scr.config import *\n",
                "from scr.data_processing import load_all_data\n",
                "from scr.dataset import ISICDataset\n",
                "from scr.transforms import get_train_transform, get_val_transform\n",
                "from scr.model import build_model, count_parameters, load_checkpoint, save_checkpoint, print_model_info\n",
                "from scr.train import train_one_epoch, create_dataloaders, get_optimizer, get_scheduler, get_criterion\n",
                "from scr.evaluate import evaluate, plot_confusion_matrix, print_classification_report, create_submission, visualize_predictions\n",
                "\n",
                "# Set random seed\n",
                "set_seed(SEED)\n",
                "\n",
                "# Print configuration\n",
                "print_config()\n",
                "\n",
                "print(f\"\\n‚úì All modules imported successfully!\")\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "ce43e27a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "LOADING DATA\n",
                        "============================================================\n",
                        "\n",
                        "=== TRAIN DATASET ===\n",
                        "Total samples: 10015\n",
                        "\n",
                        "Label distribution:\n",
                        "  AKIEC: 327 (3.27%)\n",
                        "  BCC: 514 (5.13%)\n",
                        "  BKL: 1099 (10.97%)\n",
                        "  DF: 115 (1.15%)\n",
                        "  MEL: 1113 (11.11%)\n",
                        "  NV: 6705 (66.95%)\n",
                        "  VASC: 142 (1.42%)\n",
                        "\n",
                        "=== VAL DATASET ===\n",
                        "Total samples: 193\n",
                        "\n",
                        "Label distribution:\n",
                        "  AKIEC: 8 (4.15%)\n",
                        "  BCC: 15 (7.77%)\n",
                        "  BKL: 22 (11.40%)\n",
                        "  DF: 1 (0.52%)\n",
                        "  MEL: 21 (10.88%)\n",
                        "  NV: 123 (63.73%)\n",
                        "  VASC: 3 (1.55%)\n",
                        "\n",
                        "=== TEST DATASET ===\n",
                        "Total samples: 1512\n",
                        "\n",
                        "Label distribution:\n",
                        "  AKIEC: 43 (2.84%)\n",
                        "  BCC: 93 (6.15%)\n",
                        "  BKL: 217 (14.35%)\n",
                        "  DF: 44 (2.91%)\n",
                        "  MEL: 171 (11.31%)\n",
                        "  NV: 909 (60.12%)\n",
                        "  VASC: 35 (2.31%)\n",
                        "\n",
                        "=== LABEL MAPPING ===\n",
                        "Number of classes: 7\n",
                        "  0: AKIEC\n",
                        "  1: BCC\n",
                        "  2: BKL\n",
                        "  3: DF\n",
                        "  4: MEL\n",
                        "  5: NV\n",
                        "  6: VASC\n",
                        "\n",
                        "=== CLASS IMBALANCE ANALYSIS ===\n",
                        "Class imbalance ratio: 58.30\n",
                        "Max class count: 6705\n",
                        "Min class count: 115\n",
                        "‚ö† High class imbalance detected (ratio > 3.0).\n",
                        "   Will use WeightedRandomSampler for balanced training.\n",
                        "\n",
                        "============================================================\n",
                        "DATA LOADING COMPLETE\n",
                        "============================================================\n",
                        "\n",
                        "‚úì Datasets created:\n",
                        "  Train: 10015 samples\n",
                        "  Val: 193 samples\n",
                        "  Test: 1512 samples\n",
                        "‚úì Created WeightedRandomSampler for balanced training\n",
                        "\n",
                        "============================================================\n",
                        "DATA LOADERS\n",
                        "============================================================\n",
                        "Batch size: 16\n",
                        "Num workers: 0\n",
                        "Train batches: 626\n",
                        "Val batches: 13\n",
                        "Test batches: 95\n",
                        "============================================================\n",
                        "\n",
                        "‚úì Dataloaders created:\n",
                        "  Train: 626 batches\n",
                        "  Val: 13 batches\n",
                        "  Test: 95 batches\n"
                    ]
                }
            ],
            "source": [
                "# Cell 2: Load v√† chu·∫©n b·ªã d·ªØ li·ªáu\n",
                "\n",
                "# Load all data\n",
                "df_train, df_val, df_test, label2idx, idx2label, num_classes, use_weighted_sampler = load_all_data(\n",
                "    PATH_TRAIN_CSV, PATH_VAL_CSV, PATH_TEST_CSV,\n",
                "    DIR_TRAIN_IMG, DIR_VAL_IMG, DIR_TEST_IMG\n",
                ")\n",
                "\n",
                "# Store label mappings in config (for checkpoint saving)\n",
                "LABEL2IDX = label2idx\n",
                "IDX2LABEL = idx2label\n",
                "\n",
                "# Create datasets\n",
                "train_transform = get_train_transform()\n",
                "val_transform = get_val_transform()\n",
                "\n",
                "train_dataset = ISICDataset(df_train, transform=train_transform)\n",
                "val_dataset = ISICDataset(df_val, transform=val_transform)\n",
                "test_dataset = ISICDataset(df_test, transform=val_transform)\n",
                "\n",
                "print(f\"\\n‚úì Datasets created:\")\n",
                "print(f\"  Train: {len(train_dataset)} samples\")\n",
                "print(f\"  Val: {len(val_dataset)} samples\")\n",
                "print(f\"  Test: {len(test_dataset)} samples\")\n",
                "\n",
                "# Create dataloaders\n",
                "train_loader, val_loader, test_loader = create_dataloaders(\n",
                "    df_train, df_val, df_test,\n",
                "    train_dataset, val_dataset, test_dataset,\n",
                "    BATCH_SIZE, NUM_WORKERS, use_weighted_sampler\n",
                ")\n",
                "\n",
                "print(f\"\\n‚úì Dataloaders created:\")\n",
                "print(f\"  Train: {len(train_loader)} batches\")\n",
                "print(f\"  Val: {len(val_loader)} batches\")\n",
                "print(f\"  Test: {len(test_loader)} batches\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "d45e9af6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "MODEL INFORMATION\n",
                        "============================================================\n",
                        "Model: EFFICIENTNET_B1\n",
                        "Total parameters: 6,522,151\n",
                        "Trainable parameters: 6,522,151\n",
                        "============================================================\n",
                        "‚úì Optimizer: AdamW (lr=0.0001, weight_decay=0.0001)\n",
                        "‚úì Scheduler: CosineAnnealingLR (T_max=10)\n",
                        "‚úì Loss: CrossEntropyLoss\n",
                        "\n",
                        "‚úì Model and training components initialized!\n"
                    ]
                }
            ],
            "source": [
                "# Cell 3: Kh·ªüi t·∫°o model v√† training components\n",
                "\n",
                "# Build model\n",
                "model = build_model(num_classes=num_classes, pretrained=True, model_name=MODEL_NAME)\n",
                "model = model.to(DEVICE)\n",
                "\n",
                "# Print model info\n",
                "print_model_info(model, MODEL_NAME.upper())\n",
                "\n",
                "# Setup training components\n",
                "optimizer = get_optimizer(model, LEARNING_RATE, WEIGHT_DECAY)\n",
                "scheduler = get_scheduler(optimizer, NUM_EPOCHS, USE_COSINE_SCHEDULER)\n",
                "criterion = get_criterion(USE_LABEL_SMOOTHING, LABEL_SMOOTHING)\n",
                "\n",
                "# Initialize mixed precision scaler\n",
                "from torch.cuda.amp import GradScaler\n",
                "scaler = GradScaler() if torch.cuda.is_available() else None\n",
                "\n",
                "print(\"\\n‚úì Model and training components initialized!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "3794cf6b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üÜï Starting fresh training\n",
                        "\n",
                        "‚öôÔ∏è Training Configuration:\n",
                        "  ‚Ä¢ Epochs: 10\n",
                        "  ‚Ä¢ Batch Size: 16\n",
                        "  ‚Ä¢ Validate every: 3 epochs\n",
                        "  ‚Ä¢ Save every: 2 epochs\n",
                        "  ‚Ä¢ Early Stop Patience: 2\n",
                        "\n",
                        "\n",
                        "============================================================\n",
                        "Epoch 1/10\n",
                        "============================================================\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "908b24728e34483a8a07391ffe173157",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Training:   0%|          | 0/626 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     53\u001b[39m outputs = model(images)\n\u001b[32m     54\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m optimizer.step()\n\u001b[32m     58\u001b[39m running_loss += loss.item() * images.size(\u001b[32m0\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\ISIC2018\\.venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\ISIC2018\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32md:\\ISIC2018\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "# Cell 4: Training Loop (Optimized for CPU)\n",
                "\n",
                "# Training history\n",
                "history = {\n",
                "    'epoch': [], 'train_loss': [], 'train_acc': [],\n",
                "    'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_bal_acc': [], 'lr': []\n",
                "}\n",
                "\n",
                "# Best model tracking\n",
                "best_val_f1 = 0.0\n",
                "best_epoch = 0\n",
                "patience_counter = 0\n",
                "start_epoch = 0\n",
                "\n",
                "# Load existing checkpoint if exists\n",
                "import os\n",
                "if os.path.exists(MODEL_PATH):\n",
                "    checkpoint = load_checkpoint(model, optimizer, MODEL_PATH, DEVICE)\n",
                "    start_epoch = checkpoint.get('epoch', 0)\n",
                "    best_val_f1 = checkpoint.get('best_val_f1', 0.0)\n",
                "    best_epoch = checkpoint.get('best_epoch', 0)\n",
                "    if 'history' in checkpoint:\n",
                "        history = checkpoint['history']\n",
                "    print(f\"üìÇ Resumed from epoch {start_epoch}\")\n",
                "else:\n",
                "    print(\"üÜï Starting fresh training\")\n",
                "\n",
                "print(f\"\\n‚öôÔ∏è Training Configuration:\")\n",
                "print(f\"  ‚Ä¢ Epochs: {NUM_EPOCHS}\")\n",
                "print(f\"  ‚Ä¢ Batch Size: {BATCH_SIZE}\")\n",
                "print(f\"  ‚Ä¢ Validate every: {VAL_EVERY_N_EPOCHS} epochs\")\n",
                "print(f\"  ‚Ä¢ Save every: {SAVE_EVERY_N_EPOCHS} epochs\")\n",
                "print(f\"  ‚Ä¢ Early Stop Patience: {EARLY_STOP_PATIENCE}\\n\")\n",
                "\n",
                "# Training loop\n",
                "for epoch in range(start_epoch, NUM_EPOCHS):\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
                "    print('='*60)\n",
                "    \n",
                "    # Train\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    all_preds = []\n",
                "    all_labels = []\n",
                "    \n",
                "    train_pbar = tqdm(train_loader, desc=f\"Training\", leave=False)\n",
                "    for images, labels, _ in train_pbar:\n",
                "        images = images.to(DEVICE)\n",
                "        labels = labels.to(DEVICE)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        running_loss += loss.item() * images.size(0)\n",
                "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
                "        all_preds.extend(preds)\n",
                "        all_labels.extend(labels.cpu().numpy())\n",
                "        train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "    \n",
                "    from sklearn.metrics import accuracy_score\n",
                "    train_loss = running_loss / len(train_loader.dataset)\n",
                "    train_acc = accuracy_score(all_labels, all_preds)\n",
                "    \n",
                "    print(f\"üìà Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
                "    \n",
                "    # Validate only every N epochs\n",
                "    should_validate = (epoch + 1) % VAL_EVERY_N_EPOCHS == 0 or (epoch + 1) == NUM_EPOCHS\n",
                "    \n",
                "    if should_validate:\n",
                "        print(\"üîç Running validation...\")\n",
                "        val_loss, val_acc, val_f1, val_bal_acc, _, _, _, _ = evaluate(model, val_loader, criterion, DEVICE)\n",
                "        print(f\"üìä Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
                "        \n",
                "        # Check for best model\n",
                "        if val_f1 > best_val_f1:\n",
                "            best_val_f1 = val_f1\n",
                "            best_epoch = epoch + 1\n",
                "            patience_counter = 0\n",
                "            print(f\"‚úÖ New best model! F1: {val_f1:.4f}\")\n",
                "        else:\n",
                "            patience_counter += 1\n",
                "            print(f\"‚è≥ No improvement ({patience_counter}/{EARLY_STOP_PATIENCE})\")\n",
                "        \n",
                "        # Record history\n",
                "        history['epoch'].append(epoch + 1)\n",
                "        history['train_loss'].append(train_loss)\n",
                "        history['train_acc'].append(train_acc)\n",
                "        history['val_loss'].append(val_loss)\n",
                "        history['val_acc'].append(val_acc)\n",
                "        history['val_f1'].append(val_f1)\n",
                "        history['val_bal_acc'].append(val_bal_acc)\n",
                "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
                "    \n",
                "    # Update learning rate\n",
                "    if USE_COSINE_SCHEDULER:\n",
                "        scheduler.step()\n",
                "    \n",
                "    # Save checkpoint periodically\n",
                "    should_save = (epoch + 1) % SAVE_EVERY_N_EPOCHS == 0 or (epoch + 1) == NUM_EPOCHS\n",
                "    if should_save:\n",
                "        save_checkpoint(model, optimizer, epoch + 1, best_val_f1, best_epoch,\n",
                "                       val_f1 if should_validate else 0.0, \n",
                "                       val_acc if should_validate else 0.0, \n",
                "                       history, label2idx, idx2label, num_classes, MODEL_PATH)\n",
                "        print(f\"üíæ Checkpoint saved at epoch {epoch+1}\")\n",
                "    \n",
                "    # Early stopping (only check when we validate)\n",
                "    if should_validate and patience_counter >= EARLY_STOP_PATIENCE:\n",
                "        print(f\"\\n‚ö†Ô∏è Early stopping at epoch {epoch + 1}\")\n",
                "        break\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"‚úÖ Training Complete!\")\n",
                "print(f\"üìä Best: Epoch {best_epoch} | F1: {best_val_f1:.4f}\")\n",
                "print(f\"üíæ Model saved: {MODEL_PATH}\")\n",
                "print('='*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0b5d4315",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: ƒê√°nh gi√° tr√™n Validation Set\n",
                "\n",
                "# Load best model\n",
                "print(\"Loading best model...\")\n",
                "checkpoint = load_checkpoint(model, None, MODEL_PATH, DEVICE)\n",
                "\n",
                "# Evaluate on validation set\n",
                "print(\"\\nEvaluating on validation set...\")\n",
                "val_loss, val_acc, val_f1, val_bal_acc, val_preds, val_labels, val_probs, val_image_ids = evaluate(\n",
                "    model, val_loader, criterion, DEVICE\n",
                ")\n",
                "\n",
                "print(f\"\\nValidation Results:\")\n",
                "print(f\"  Loss: {val_loss:.4f}\")\n",
                "print(f\"  Accuracy: {val_acc:.4f}\")\n",
                "print(f\"  Macro F1: {val_f1:.4f}\")\n",
                "print(f\"  Balanced Accuracy: {val_bal_acc:.4f}\")\n",
                "\n",
                "# Confusion Matrix\n",
                "plot_confusion_matrix(\n",
                "    val_labels, val_preds, idx2label,\n",
                "    save_path=os.path.join(DIR_FIGURES, 'val_confusion_matrix.png'),\n",
                "    title='Validation Confusion Matrix'\n",
                ")\n",
                "\n",
                "# Classification Report\n",
                "print_classification_report(val_labels, val_preds, idx2label)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ce006b00",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: ƒê√°nh gi√° tr√™n Test Set\n",
                "\n",
                "# Evaluate on test set\n",
                "print(\"Evaluating on test set...\")\n",
                "test_loss, test_acc, test_f1, test_bal_acc, test_preds, test_labels, test_probs, test_image_ids = evaluate(\n",
                "    model, test_loader, criterion, DEVICE\n",
                ")\n",
                "\n",
                "print(f\"\\nTest Results:\")\n",
                "print(f\"  Loss: {test_loss:.4f}\")\n",
                "print(f\"  Accuracy: {test_acc:.4f}\")\n",
                "print(f\"  Macro F1: {test_f1:.4f}\")\n",
                "print(f\"  Balanced Accuracy: {test_bal_acc:.4f}\")\n",
                "\n",
                "# Confusion Matrix\n",
                "plot_confusion_matrix(\n",
                "    test_labels, test_preds, idx2label,\n",
                "    save_path=os.path.join(DIR_FIGURES, 'test_confusion_matrix.png'),\n",
                "    title='Test Confusion Matrix'\n",
                ")\n",
                "\n",
                "# Classification Report\n",
                "print_classification_report(test_labels, test_preds, idx2label)\n",
                "\n",
                "# Create submission\n",
                "submission_path = os.path.join(DIR_SUBMISSIONS, 'test_predictions.csv')\n",
                "submission_df = create_submission(test_image_ids, test_preds, test_probs, idx2label, submission_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0b447273",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Inference Demo\n",
                "\n",
                "# Demo images\n",
                "demo_images = [\n",
                "    (os.path.join(DIR_TEST_IMG, \"ISIC_0034524.jpg\"), \"ISIC_0034524\"),\n",
                "    (os.path.join(DIR_VAL_IMG, \"ISIC_0034321.jpg\"), \"ISIC_0034321\"),\n",
                "]\n",
                "\n",
                "# Visualize predictions\n",
                "visualize_predictions(\n",
                "    model, demo_images, val_transform, DEVICE, idx2label,\n",
                "    save_path=os.path.join(DIR_FIGURES, 'inference_demo.png'),\n",
                "    top_k=3\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ Inference demo completed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3c189407",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: V·∫Ω Training History\n",
                "\n",
                "if len(history['epoch']) > 0:\n",
                "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
                "    \n",
                "    # Loss\n",
                "    axes[0, 0].plot(history['epoch'], history['train_loss'], label='Train Loss', marker='o')\n",
                "    axes[0, 0].plot(history['epoch'], history['val_loss'], label='Val Loss', marker='s')\n",
                "    axes[0, 0].set_xlabel('Epoch')\n",
                "    axes[0, 0].set_ylabel('Loss')\n",
                "    axes[0, 0].set_title('Training and Validation Loss')\n",
                "    axes[0, 0].legend()\n",
                "    axes[0, 0].grid(True, alpha=0.3)\n",
                "    \n",
                "    # Accuracy\n",
                "    axes[0, 1].plot(history['epoch'], history['train_acc'], label='Train Acc', marker='o')\n",
                "    axes[0, 1].plot(history['epoch'], history['val_acc'], label='Val Acc', marker='s')\n",
                "    axes[0, 1].set_xlabel('Epoch')\n",
                "    axes[0, 1].set_ylabel('Accuracy')\n",
                "    axes[0, 1].set_title('Training and Validation Accuracy')\n",
                "    axes[0, 1].legend()\n",
                "    axes[0, 1].grid(True, alpha=0.3)\n",
                "    \n",
                "    # F1 Score\n",
                "    axes[1, 0].plot(history['epoch'], history['val_f1'], label='Val F1', marker='s', color='green')\n",
                "    axes[1, 0].set_xlabel('Epoch')\n",
                "    axes[1, 0].set_ylabel('F1 Score')\n",
                "    axes[1, 0].set_title('Validation F1 Score')\n",
                "    axes[1, 0].legend()\n",
                "    axes[1, 0].grid(True, alpha=0.3)\n",
                "    \n",
                "    # Learning Rate\n",
                "    axes[1, 1].plot(history['epoch'], history['lr'], label='Learning Rate', marker='o', color='red')\n",
                "    axes[1, 1].set_xlabel('Epoch')\n",
                "    axes[1, 1].set_ylabel('Learning Rate')\n",
                "    axes[1, 1].set_title('Learning Rate Schedule')\n",
                "    axes[1, 1].set_yscale('log')\n",
                "    axes[1, 1].legend()\n",
                "    axes[1, 1].grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(os.path.join(DIR_FIGURES, 'training_history.png'), dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"‚úì Training history plotted and saved!\")\n",
                "else:\n",
                "    print(\"No training history available.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
