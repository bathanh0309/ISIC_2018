{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bathanh0309/ISIC_2018/blob/main/main_ggColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Bgmysv1O975",
      "metadata": {
        "id": "3Bgmysv1O975"
      },
      "source": [
        "### ISIC 2018 Skin Lesion Classification - EfficientNet B1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 1: Mount Google Drive\n",
        "# ============================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"CELL 1: MOUNT GOOGLE DRIVE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/ISIC_2018\"\n",
        "\n",
        "if os.path.exists(DRIVE_ROOT):\n",
        "    print(f\"‚úì Found project folder: {DRIVE_ROOT}\")\n",
        "    print(f\"  Contents: {os.listdir(DRIVE_ROOT)}\")\n",
        "else:\n",
        "    print(f\"‚ùå Project folder NOT found: {DRIVE_ROOT}\")\n",
        "    print(\"  Please create this folder and upload your dataset!\")\n"
      ],
      "metadata": {
        "id": "LTlSybTlI-QH"
      },
      "id": "LTlSybTlI-QH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 2: Install Dependencies & Check GPU\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CELL 2: INSTALL DEPENDENCIES & CHECK GPU\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Install required packages (most are pre-installed on Colab)\n",
        "print(\"\\n Installing required packages...\")\n",
        "!pip install -q timm>=0.9.0 tqdm scikit-learn seaborn pillow\n",
        "\n",
        "print(\"‚úì Packages installed!\")\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"\\n System Info:\")\n",
        "print(f\"  PyTorch version: {torch.__version__}\")\n",
        "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    !nvidia-smi\n",
        "else:\n",
        "    print(\"\\n WARNING: No GPU detected! Training will be slow on CPU.\")\n",
        "    print(\"  Go to Runtime > Change runtime type > GPU\")\n"
      ],
      "metadata": {
        "id": "BkADDujUKlF9"
      },
      "id": "BkADDujUKlF9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 3: Setup Python Path & Import Modules\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CELL 3: SETUP PYTHON PATH & IMPORT MODULES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add src folder to Python path\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/ISIC_2018\"\n",
        "SRC_PATH = os.path.join(DRIVE_ROOT, \"src\")\n",
        "\n",
        "if SRC_PATH not in sys.path:\n",
        "    sys.path.insert(0, SRC_PATH)\n",
        "    print(f\"‚úì Added to Python path: {SRC_PATH}\")\n",
        "\n",
        "# Verify src folder\n",
        "if os.path.exists(SRC_PATH):\n",
        "    print(f\"‚úì src folder found!\")\n",
        "    print(f\"  Files: {os.listdir(SRC_PATH)}\")\n",
        "else:\n",
        "    print(f\" src folder NOT found at: {SRC_PATH}\")\n",
        "    print(\"  Please upload your src folder to Google Drive!\")\n",
        "\n",
        "# Import custom modules\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from config import *\n",
        "from data_processing import load_all_data\n",
        "from dataset import ISICDataset\n",
        "from transforms import get_train_transform, get_val_transform\n",
        "from model import build_model, count_parameters, load_checkpoint, save_checkpoint, print_model_info\n",
        "from train import train_one_epoch, create_dataloaders, get_optimizer, get_scheduler, get_criterion\n",
        "from evaluate import evaluate, plot_confusion_matrix, print_classification_report, create_submission, visualize_predictions\n",
        "from visualize_dataset import plot_dataset_overview, plot_class_distribution_comparison\n",
        "\n",
        "# Set random seed\n",
        "set_seed(SEED)\n",
        "\n",
        "# Print configuration\n",
        "print_config()\n",
        "\n",
        "print(f\"\\n‚úì All modules imported successfully!\")\n"
      ],
      "metadata": {
        "id": "EoutP1ggKtw-"
      },
      "id": "EoutP1ggKtw-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# CELL 4: Load and Visualize Dataset\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CELL 4: LOAD AND VISUALIZE DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load all data\n",
        "df_train, df_val, df_test, label2idx, idx2label, num_classes, use_weighted_sampler = load_all_data(\n",
        "    PATH_TRAIN_CSV, PATH_VAL_CSV, PATH_TEST_CSV,\n",
        "    DIR_TRAIN_IMG, DIR_VAL_IMG, DIR_TEST_IMG\n",
        ")\n",
        "\n",
        "# Store label mappings\n",
        "LABEL2IDX = label2idx\n",
        "IDX2LABEL = idx2label\n",
        "\n",
        "# Visualize dataset\n",
        "print(\"\\n Generating dataset visualizations...\")\n",
        "fig1 = plot_dataset_overview(df_train, df_val, df_test,\n",
        "                              save_path=os.path.join(DIR_FIGURES, \"dataset_overview.png\"))\n",
        "plt.show()\n",
        "\n",
        "fig2 = plot_class_distribution_comparison(df_train, df_val, df_test,\n",
        "                                          save_path=os.path.join(DIR_FIGURES, \"class_distribution.png\"))\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úì Visualizations saved to: {DIR_FIGURES}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2byQPd_mKzsk"
      },
      "id": "2byQPd_mKzsk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================\n",
        "# CELL 5: Create Datasets and DataLoaders\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CELL 5: CREATE DATASETS AND DATALOADERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create transforms\n",
        "train_transform = get_train_transform()\n",
        "val_transform = get_val_transform()\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ISICDataset(df_train, transform=train_transform)\n",
        "val_dataset = ISICDataset(df_val, transform=val_transform)\n",
        "test_dataset = ISICDataset(df_test, transform=val_transform)\n",
        "\n",
        "print(f\"\\n‚úì Datasets created:\")\n",
        "print(f\"  Train: {len(train_dataset)} samples\")\n",
        "print(f\"  Val: {len(val_dataset)} samples\")\n",
        "print(f\"  Test: {len(test_dataset)} samples\")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader, val_loader, test_loader = create_dataloaders(\n",
        "    df_train, df_val, df_test,\n",
        "    train_dataset, val_dataset, test_dataset,\n",
        "    BATCH_SIZE, NUM_WORKERS, use_weighted_sampler\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Dataloaders created:\")\n",
        "print(f\"  Train: {len(train_loader)} batches\")\n",
        "print(f\"  Val: {len(val_loader)} batches\")\n",
        "print(f\"  Test: {len(test_loader)} batches\")\n",
        "\n"
      ],
      "metadata": {
        "id": "z7EQW38gK7ZV"
      },
      "id": "z7EQW38gK7ZV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 6: Build Model (Transfer Learning)\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CELL 6: BUILD MODEL (TRANSFER LEARNING)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Build model with frozen backbone for transfer learning\n",
        "print(f\"\\nüîß Training Mode: {'Transfer Learning (frozen backbone)' if FREEZE_BACKBONE else 'Full Training'}\")\n",
        "\n",
        "model = build_model(\n",
        "    num_classes=num_classes,\n",
        "    pretrained=True,\n",
        "    model_name=MODEL_NAME,\n",
        "    drop_rate=DROP_RATE,\n",
        "    drop_path_rate=DROP_PATH_RATE,\n",
        "    freeze_backbone=FREEZE_BACKBONE  # Freeze backbone for transfer learning\n",
        ")\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Print model info\n",
        "print_model_info(model, MODEL_NAME.upper())\n",
        "\n",
        "# Setup training components\n",
        "optimizer = get_optimizer(model, LEARNING_RATE, WEIGHT_DECAY)\n",
        "scheduler = get_scheduler(optimizer, NUM_EPOCHS, USE_COSINE_SCHEDULER)\n",
        "criterion = get_criterion(USE_LABEL_SMOOTHING, LABEL_SMOOTHING)\n",
        "\n",
        "# Initialize tracking - Fresh start\n",
        "history = {\n",
        "    'epoch': [], 'train_loss': [], 'train_acc': [],\n",
        "    'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_bal_acc': [], 'lr': []\n",
        "}\n",
        "best_val_f1 = 0.0\n",
        "best_epoch = 0\n",
        "start_epoch = 0\n",
        "\n",
        "# Check for existing checkpoint (optional resume)\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    print(f\"\\nüîÑ Found existing checkpoint: {MODEL_PATH}\")\n",
        "    try:\n",
        "        checkpoint = load_checkpoint(model, optimizer, MODEL_PATH, DEVICE)\n",
        "        start_epoch = checkpoint.get('epoch', 0)\n",
        "        best_val_f1 = checkpoint.get('best_val_f1', 0.0)\n",
        "        best_epoch = checkpoint.get('best_epoch', 0)\n",
        "        if 'history' in checkpoint:\n",
        "            history = checkpoint['history']\n",
        "        print(f\"‚úì Resumed from epoch {start_epoch}, best F1: {best_val_f1:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Could not load checkpoint: {e}\")\n",
        "        print(\"  Starting fresh training...\")\n",
        "else:\n",
        "    print(f\"\\n‚úì No checkpoint found. Starting fresh training.\")\n",
        "    print(f\"  Model will be saved to: {MODEL_PATH}\")\n",
        "\n",
        "print(\"\\n‚úì Model and training components ready!\")"
      ],
      "metadata": {
        "id": "qJ9s9fXjMQzW"
      },
      "id": "qJ9s9fXjMQzW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import config\n",
        "import importlib\n",
        "importlib.reload(config) # L·ªánh n√†y √©p Python ƒë·ªçc l·∫°i file config.py t·ª´ Drive\n",
        "print(f\"M·ª•c ti√™u m·ªõi: {config.NUM_EPOCHS} epochs\")"
      ],
      "metadata": {
        "id": "GDc0zir1c5JM"
      },
      "id": "GDc0zir1c5JM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 7: Training Loop\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CELL 7: TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from engine import train_model\n",
        "import config\n",
        "\n",
        "# T√≠nh to√°n s·ªë l∆∞·ª£ng epoch c√≤n thi·∫øu ƒë·ªÉ ƒë·∫°t m·ªëc 5\n",
        "remaining_epochs = config.NUM_EPOCHS - start_epoch\n",
        "\n",
        "if remaining_epochs <= 0:\n",
        "    print(f\"‚úÖ ƒê√£ ho√†n th√†nh: Model ƒë√£ ·ªü m·ªëc Epoch {start_epoch}.\")\n",
        "    print(\"‚è≠Ô∏è Kh√¥ng c·∫ßn hu·∫•n luy·ªán th√™m. B·∫°n c√≥ th·ªÉ chuy·ªÉn sang Cell 8.\")\n",
        "else:\n",
        "    print(f\"üîÑ Tr·∫°ng th√°i: ƒê√£ c√≥ {start_epoch} epoch. M·ª•c ti√™u: {config.NUM_EPOCHS} epoch.\")\n",
        "    print(f\"üöÄ H√†nh ƒë·ªông: S·∫Ω hu·∫•n luy·ªán th√™m {remaining_epochs} epoch n·ªØa (Epoch 5).\")\n",
        "\n",
        "    cfg_dict = {\n",
        "        'VAL_EVERY_N_EPOCHS': config.VAL_EVERY_N_EPOCHS,\n",
        "        'SAVE_EVERY_N_EPOCHS': config.SAVE_EVERY_N_EPOCHS,\n",
        "        'EARLY_STOP_PATIENCE': config.EARLY_STOP_PATIENCE,\n",
        "        'USE_COSINE_SCHEDULER': config.USE_COSINE_SCHEDULER,\n",
        "        'USE_TTA_VALIDATION': config.USE_TTA_VALIDATION,\n",
        "        'MODEL_PATH': config.MODEL_PATH,\n",
        "        'NUM_CLASSES': config.NUM_CLASSES,\n",
        "        'label2idx': label2idx,\n",
        "        'idx2label': idx2label\n",
        "    }\n",
        "\n",
        "    # Ch·∫°y hu·∫•n luy·ªán\n",
        "    model, history, best_val_f1, best_epoch = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "        num_epochs=remaining_epochs,\n",
        "        device=DEVICE,\n",
        "        config_dict=cfg_dict,\n",
        "        start_epoch=start_epoch,\n",
        "        best_val_f1=best_val_f1,\n",
        "        best_epoch=best_epoch,\n",
        "        history=history\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüéâ Ho√†n th√†nh m·ª•c ti√™u nghi√™n c·ª©u giai ƒëo·∫°n 1 (5 Epochs)!\")"
      ],
      "metadata": {
        "id": "q84KooiQMSjj"
      },
      "id": "q84KooiQMSjj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 8: Evaluate on Validation Set\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CELL 8: EVALUATE ON VALIDATION SET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Load best model v√† n·∫°p l·∫°i History\n",
        "print(f\"Loading best model from: {MODEL_PATH}\")\n",
        "# ƒê·∫£m b·∫£o n·∫°p history t·ª´ checkpoint ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì ·ªü Cell 10\n",
        "checkpoint = load_checkpoint(model, None, MODEL_PATH, DEVICE)\n",
        "if 'history' in checkpoint:\n",
        "    history = checkpoint['history']\n",
        "    print(f\"‚úì ƒê√£ kh√¥i ph·ª•c l·ªãch s·ª≠ hu·∫•n luy·ªán ({len(history['epoch'])} epochs)\")\n",
        "\n",
        "# 2. Ch·∫°y ƒë√°nh gi√°\n",
        "print(\"\\nEvaluating on validation set...\")\n",
        "val_loss, val_acc, val_f1, val_bal_acc, val_preds, val_labels, val_probs, val_image_ids = evaluate(\n",
        "    model, val_loader, criterion, DEVICE, use_tta=USE_TTA_VALIDATION\n",
        ")\n",
        "\n",
        "# 3. Hi·ªÉn th·ªã k·∫øt qu·∫£ d·∫°ng s·ªë\n",
        "print(f\"\\nüìä Validation Results (Epoch {checkpoint.get('epoch', 'N/A')}):\")\n",
        "print(f\"  ‚óè Loss: {val_loss:.4f}\")\n",
        "print(f\"  ‚óè Accuracy (Raw): {val_acc:.4f}\")\n",
        "print(f\"  ‚óè Balanced Accuracy: {val_bal_acc:.4f}\")\n",
        "print(f\"  ‚óè Macro F1-Score: {val_f1:.4f}  <-- Ch·ªâ s·ªë quan tr·ªçng nh·∫•t\")\n",
        "\n",
        "# 4. Tr·ª±c quan h√≥a Confusion Matrix\n",
        "# L∆∞u √Ω: Confusion Matrix gi√∫p b·∫°n bi·∫øt m√¥ h√¨nh ƒëang nh·∫ßm l·∫´n ·ªü ƒë√¢u (V√≠ d·ª•: nh·∫ßm Melanoma sang NV)\n",
        "plot_confusion_matrix(\n",
        "    val_labels, val_preds, idx2label,\n",
        "    save_path=os.path.join(DIR_FIGURES, 'val_confusion_matrix.png'),\n",
        "    title=f'Validation Confusion Matrix (Epoch {checkpoint.get(\"epoch\")})'\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "# 5. In b√°o c√°o chi ti·∫øt t·ª´ng l·ªõp (Precision, Recall, F1 cho t·ª´ng lo·∫°i b·ªánh)\n",
        "report = print_classification_report(val_labels, val_preds, idx2label)"
      ],
      "metadata": {
        "id": "ntiC8MpNMiL6"
      },
      "id": "ntiC8MpNMiL6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 9: Training History Visualization\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CELL 10: TRAINING HISTORY VISUALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if 'history' in locals() and len(history.get('epoch', [])) > 0:\n",
        "    plt.style.use('seaborn-v0_8-whitegrid') # L√†m bi·ªÉu ƒë·ªì ƒë·∫πp h∆°n\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # ƒê·ªì th·ªã Loss (H·ªçc m√°y t·ªët l√† Loss gi·∫£m d·∫ßn)\n",
        "    axes[0].plot(history['epoch'], history['train_loss'], label='Train Loss', marker='o', color='#1f77b4')\n",
        "    axes[0].plot(history['epoch'], history['val_loss'], label='Val Loss', marker='s', color='#ff7f0e')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Training & Validation Loss')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # ƒê·ªì th·ªã Accuracy\n",
        "    axes[1].plot(history['epoch'], history['train_acc'], label='Train Acc', marker='o', color='#2ca02c')\n",
        "    axes[1].plot(history['epoch'], history['val_acc'], label='Val Acc', marker='s', color='#d62728')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].set_title('Training & Validation Accuracy')\n",
        "    axes[1].legend()\n",
        "\n",
        "    # ƒê·ªì th·ªã F1 Score (Ch·ªâ s·ªë ƒë√°nh gi√° ƒë·ªô v∆∞·ª£t tr·ªôi)\n",
        "    axes[2].plot(history['epoch'], history['val_f1'], label='Val F1', color='#9467bd', marker='D', linewidth=2)\n",
        "    if 'best_epoch' in locals() and best_epoch > 0:\n",
        "        axes[2].axvline(x=best_epoch, color='r', linestyle='--', label=f'Best Model (Ep {best_epoch})')\n",
        "\n",
        "    axes[2].set_xlabel('Epoch')\n",
        "    axes[2].set_ylabel('F1 Score')\n",
        "    axes[2].set_title('Validation Macro F1 Score')\n",
        "    axes[2].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    history_img_path = os.path.join(DIR_FIGURES, 'training_history_final.png')\n",
        "    plt.savefig(history_img_path, dpi=200)\n",
        "    plt.show()\n",
        "    print(f\"‚úì Bi·ªÉu ƒë·ªì l·ªãch s·ª≠ ƒë√£ l∆∞u t·∫°i: {history_img_path}\")\n",
        "else:\n",
        "    print(\"‚ö† C·∫£nh b√°o: Bi·∫øn 'history' tr·ªëng. H√£y ch·∫°y Cell 8 tr∆∞·ªõc ƒë·ªÉ load history t·ª´ checkpoint.\")"
      ],
      "metadata": {
        "id": "1DRN8RGkMlqX"
      },
      "id": "1DRN8RGkMlqX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 11: D·ª± ƒëo√°n ·∫£nh\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_test_results(model, dataset, device, idx2label, num_images=6):\n",
        "    model.eval()\n",
        "\n",
        "    num_images = min(num_images, len(dataset))\n",
        "    indices = np.random.choice(len(dataset), num_images, replace=False)\n",
        "\n",
        "    cols = 4\n",
        "    rows = math.ceil(num_images / cols)\n",
        "\n",
        "    plt.figure(figsize=(4 * cols, 4 * rows))\n",
        "\n",
        "    # Th√¥ng s·ªë chu·∫©n ImageNet\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    print(f\"ƒêang d·ª± ƒëo√°n {num_images} ·∫£nh t·ª´ t·∫≠p Test...\")\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        # image, label (tensor), image_id\n",
        "        image, label, image_id = dataset[idx]\n",
        "\n",
        "        input_img = image.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_img)\n",
        "            pred = torch.argmax(output, dim=1).item()\n",
        "\n",
        "        img = image.permute(1, 2, 0).cpu().numpy()\n",
        "        img = std * img + mean\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # S·ª¨A L·ªñI T·∫†I ƒê√ÇY: D√πng label.item() ƒë·ªÉ chuy·ªÉn tensor sang int\n",
        "        true_label_idx = label.item()\n",
        "        is_correct = (true_label_idx == pred)\n",
        "\n",
        "        title_color = \"green\" if is_correct else \"red\"\n",
        "\n",
        "        gt_name = idx2label[true_label_idx]\n",
        "        pred_name = idx2label[pred]\n",
        "\n",
        "        plt.title(\n",
        "            f\"ID: {image_id}\\nGT: {gt_name}\\nPred: {pred_name}\",\n",
        "            color=title_color,\n",
        "            fontsize=10,\n",
        "            fontweight='bold'\n",
        "        )\n",
        "\n",
        "    plt.suptitle(\"ISIC 2018 TEST SET PREDICTIONS\\n(Gree: ƒê√∫ng | Red: Sai)\", fontsize=20, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# G·ªçi h√†m hi·ªÉn th·ªã\n",
        "visualize_test_results(\n",
        "    model,\n",
        "    test_dataset,\n",
        "    DEVICE,\n",
        "    idx2label,\n",
        "    num_images=12\n",
        ")"
      ],
      "metadata": {
        "id": "5AgQV-9hMoUP"
      },
      "id": "5AgQV-9hMoUP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss gi√∫p t·∫≠p trung v√†o c√°c m·∫´u kh√≥ h·ªçc (Hard examples)\n",
        "    v√† gi·∫£m nh·∫π ·∫£nh h∆∞·ªüng c·ªßa c√°c m·∫´u d·ªÖ (nh∆∞ n·ªët ru·ªìi th√¥ng th∆∞·ªùng - NV).\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # T√≠nh Cross Entropy c∆° b·∫£n\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss) # X√°c su·∫•t d·ª± ƒëo√°n ƒë√∫ng cho m·∫´u ƒë√≥\n",
        "\n",
        "        # C√¥ng th·ª©c Focal Loss: FL = alpha * (1 - pt)^gamma * CE\n",
        "        focal_loss = self.alpha * (1 - pt)**self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        return focal_loss.sum()"
      ],
      "metadata": {
        "id": "--ZxlN2timx5"
      },
      "id": "--ZxlN2timx5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 12: Ch·∫°y giai ƒëo·∫°n 2\n",
        "# ============================================================\n",
        "import config\n",
        "import importlib\n",
        "from model import unfreeze_backbone_layers, print_model_info, load_checkpoint\n",
        "from train import get_optimizer, get_scheduler\n",
        "\n",
        "importlib.reload(config)\n",
        "\n",
        "# 1. N·∫°p l·∫°i m√¥ h√¨nh t·∫°i m·ªëc Epoch 5 ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng b·ªã m·∫•t d·ªØ li·ªáu v·ª´a train\n",
        "checkpoint = load_checkpoint(model, optimizer, config.MODEL_PATH, DEVICE)\n",
        "start_epoch = checkpoint.get('epoch', 5)\n",
        "\n",
        "# 2. C√†i ƒë·∫∑t m·ª•c ti√™u d·ª´ng l·∫°i ·ªü Epoch 6\n",
        "NEW_TOTAL_EPOCHS = 6\n",
        "remaining_epochs = NEW_TOTAL_EPOCHS - start_epoch\n",
        "\n",
        "if remaining_epochs <= 0:\n",
        "    print(f\"‚úÖ ƒê√£ ƒë·ªß {start_epoch} Epoch. B·∫°n c√≥ th·ªÉ l√†m b√°o c√°o ngay!\")\n",
        "else:\n",
        "    print(f\"üîÑ ƒêang ·ªü Epoch {start_epoch}. S·∫Ω ch·∫°y n·ªët {remaining_epochs} Epoch cu·ªëi.\")\n",
        "\n",
        "    # 3. Gi·ªØ nguy√™n c·∫•u h√¨nh Fine-tuning\n",
        "    unfreeze_backbone_layers(model, num_layers=-1)\n",
        "    optimizer = get_optimizer(model, lr=1e-5, weight_decay=config.WEIGHT_DECAY)\n",
        "    scheduler = get_scheduler(optimizer, num_epochs=remaining_epochs)\n",
        "    criterion = FocalLoss(gamma=2.0)\n",
        "\n",
        "    # 4. Ch·∫°y Training n·ªët v√≤ng cu·ªëi\n",
        "    model, history, best_val_f1, best_epoch = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "        num_epochs=remaining_epochs,\n",
        "        device=DEVICE,\n",
        "        config_dict=cfg_dict,\n",
        "        start_epoch=start_epoch,\n",
        "        best_val_f1=best_val_f1,\n",
        "        best_epoch=best_epoch,\n",
        "        history=history\n",
        "    )"
      ],
      "metadata": {
        "id": "BqDc15vvi3b0"
      },
      "id": "BqDc15vvi3b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 8 part 2: Evaluate on Validation Set\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CELL 8: EVALUATE ON VALIDATION SET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Load best model v√† n·∫°p l·∫°i History\n",
        "print(f\"Loading best model from: {MODEL_PATH}\")\n",
        "# ƒê·∫£m b·∫£o n·∫°p history t·ª´ checkpoint ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì ·ªü Cell 10\n",
        "checkpoint = load_checkpoint(model, None, MODEL_PATH, DEVICE)\n",
        "if 'history' in checkpoint:\n",
        "    history = checkpoint['history']\n",
        "    print(f\"‚úì ƒê√£ kh√¥i ph·ª•c l·ªãch s·ª≠ hu·∫•n luy·ªán ({len(history['epoch'])} epochs)\")\n",
        "\n",
        "# 2. Ch·∫°y ƒë√°nh gi√°\n",
        "print(\"\\nEvaluating on validation set...\")\n",
        "val_loss, val_acc, val_f1, val_bal_acc, val_preds, val_labels, val_probs, val_image_ids = evaluate(\n",
        "    model, val_loader, criterion, DEVICE, use_tta=USE_TTA_VALIDATION\n",
        ")\n",
        "\n",
        "# 3. Hi·ªÉn th·ªã k·∫øt qu·∫£ d·∫°ng s·ªë\n",
        "print(f\"\\nüìä Validation Results (Epoch {checkpoint.get('epoch', 'N/A')}):\")\n",
        "print(f\"  ‚óè Loss: {val_loss:.4f}\")\n",
        "print(f\"  ‚óè Accuracy (Raw): {val_acc:.4f}\")\n",
        "print(f\"  ‚óè Balanced Accuracy: {val_bal_acc:.4f}\")\n",
        "print(f\"  ‚óè Macro F1-Score: {val_f1:.4f}  <-- Ch·ªâ s·ªë quan tr·ªçng nh·∫•t\")\n",
        "\n",
        "# 4. Tr·ª±c quan h√≥a Confusion Matrix\n",
        "# L∆∞u √Ω: Confusion Matrix gi√∫p b·∫°n bi·∫øt m√¥ h√¨nh ƒëang nh·∫ßm l·∫´n ·ªü ƒë√¢u (V√≠ d·ª•: nh·∫ßm Melanoma sang NV)\n",
        "plot_confusion_matrix(\n",
        "    val_labels, val_preds, idx2label,\n",
        "    save_path=os.path.join(DIR_FIGURES, 'val_confusion_matrix.png'),\n",
        "    title=f'Validation Confusion Matrix (Epoch {checkpoint.get(\"epoch\")})'\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "# 5. In b√°o c√°o chi ti·∫øt t·ª´ng l·ªõp (Precision, Recall, F1 cho t·ª´ng lo·∫°i b·ªánh)\n",
        "report = print_classification_report(val_labels, val_preds, idx2label)"
      ],
      "metadata": {
        "id": "m3zYuGH91GrF"
      },
      "id": "m3zYuGH91GrF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 9 part 2: Training History Visualization\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CELL 10: TRAINING HISTORY VISUALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if 'history' in locals() and len(history.get('epoch', [])) > 0:\n",
        "    plt.style.use('seaborn-v0_8-whitegrid') # L√†m bi·ªÉu ƒë·ªì ƒë·∫πp h∆°n\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # ƒê·ªì th·ªã Loss (H·ªçc m√°y t·ªët l√† Loss gi·∫£m d·∫ßn)\n",
        "    axes[0].plot(history['epoch'], history['train_loss'], label='Train Loss', marker='o', color='#1f77b4')\n",
        "    axes[0].plot(history['epoch'], history['val_loss'], label='Val Loss', marker='s', color='#ff7f0e')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Training & Validation Loss')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # ƒê·ªì th·ªã Accuracy\n",
        "    axes[1].plot(history['epoch'], history['train_acc'], label='Train Acc', marker='o', color='#2ca02c')\n",
        "    axes[1].plot(history['epoch'], history['val_acc'], label='Val Acc', marker='s', color='#d62728')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].set_title('Training & Validation Accuracy')\n",
        "    axes[1].legend()\n",
        "\n",
        "    # ƒê·ªì th·ªã F1 Score (Ch·ªâ s·ªë ƒë√°nh gi√° ƒë·ªô v∆∞·ª£t tr·ªôi)\n",
        "    axes[2].plot(history['epoch'], history['val_f1'], label='Val F1', color='#9467bd', marker='D', linewidth=2)\n",
        "    if 'best_epoch' in locals() and best_epoch > 0:\n",
        "        axes[2].axvline(x=best_epoch, color='r', linestyle='--', label=f'Best Model (Ep {best_epoch})')\n",
        "\n",
        "    axes[2].set_xlabel('Epoch')\n",
        "    axes[2].set_ylabel('F1 Score')\n",
        "    axes[2].set_title('Validation Macro F1 Score')\n",
        "    axes[2].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    history_img_path = os.path.join(DIR_FIGURES, 'training_history_final.png')\n",
        "    plt.savefig(history_img_path, dpi=200)\n",
        "    plt.show()\n",
        "    print(f\"‚úì Bi·ªÉu ƒë·ªì l·ªãch s·ª≠ ƒë√£ l∆∞u t·∫°i: {history_img_path}\")\n",
        "else:\n",
        "    print(\"‚ö† C·∫£nh b√°o: Bi·∫øn 'history' tr·ªëng. H√£y ch·∫°y Cell 8 tr∆∞·ªõc ƒë·ªÉ load history t·ª´ checkpoint.\")"
      ],
      "metadata": {
        "id": "hWVO3oju1Pvo"
      },
      "id": "hWVO3oju1Pvo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 11: D·ª± ƒëo√°n ·∫£nh\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_test_results(model, dataset, device, idx2label, num_images=6):\n",
        "    model.eval()\n",
        "\n",
        "    num_images = min(num_images, len(dataset))\n",
        "    indices = np.random.choice(len(dataset), num_images, replace=False)\n",
        "\n",
        "    cols = 4\n",
        "    rows = math.ceil(num_images / cols)\n",
        "\n",
        "    plt.figure(figsize=(4 * cols, 4 * rows))\n",
        "\n",
        "    # Th√¥ng s·ªë chu·∫©n ImageNet\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    print(f\"ƒêang d·ª± ƒëo√°n {num_images} ·∫£nh t·ª´ t·∫≠p Test...\")\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        # image, label (tensor), image_id\n",
        "        image, label, image_id = dataset[idx]\n",
        "\n",
        "        input_img = image.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_img)\n",
        "            pred = torch.argmax(output, dim=1).item()\n",
        "\n",
        "        img = image.permute(1, 2, 0).cpu().numpy()\n",
        "        img = std * img + mean\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # S·ª¨A L·ªñI T·∫†I ƒê√ÇY: D√πng label.item() ƒë·ªÉ chuy·ªÉn tensor sang int\n",
        "        true_label_idx = label.item()\n",
        "        is_correct = (true_label_idx == pred)\n",
        "\n",
        "        title_color = \"green\" if is_correct else \"red\"\n",
        "\n",
        "        gt_name = idx2label[true_label_idx]\n",
        "        pred_name = idx2label[pred]\n",
        "\n",
        "        plt.title(\n",
        "            f\"ID: {image_id}\\nGT: {gt_name}\\nPred: {pred_name}\",\n",
        "            color=title_color,\n",
        "            fontsize=10,\n",
        "            fontweight='bold'\n",
        "        )\n",
        "\n",
        "    plt.suptitle(\"ISIC 2018 TEST SET PREDICTIONS\\n(Gree: ƒê√∫ng | Red: Sai)\", fontsize=20, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# G·ªçi h√†m hi·ªÉn th·ªã\n",
        "visualize_test_results(\n",
        "    model,\n",
        "    test_dataset,\n",
        "    DEVICE,\n",
        "    idx2label,\n",
        "    num_images=12\n",
        ")"
      ],
      "metadata": {
        "id": "exGYG7N81e_8"
      },
      "id": "exGYG7N81e_8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELL 13: FINAL TEST EVALUATION & CONFUSION MATRIX\n",
        "# ============================================================\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "# --- H√ÄM V·∫º CONFUSION MATRIX TR·ª∞C TI·∫æP ---\n",
        "def plot_final_cm(y_true, y_pred, classes, save_path, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.title(title, fontsize=14, pad=20)\n",
        "    plt.ylabel('Nh√£n th·∫≠t (Ground Truth)')\n",
        "    plt.xlabel('Nh√£n d·ª± ƒëo√°n (Prediction)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CELL 13: TEST SET EVALUATION (STAGE 2 COMPLETE)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. ƒê√°nh gi√° m√¥ h√¨nh\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"üöÄ ƒêang qu√©t {len(test_dataset)} ·∫£nh t·∫≠p Test...\")\n",
        "with torch.no_grad():\n",
        "    for images, labels, _ in test_loader:\n",
        "        images = images.to(DEVICE)\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# 2. T√≠nh to√°n ch·ªâ s·ªë\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "test_bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
        "\n",
        "print(f\"\\n‚úÖ K·∫æT QU·∫¢ CU·ªêI C√ôNG:\")\n",
        "print(f\"  ‚óè Accuracy: {test_acc:.4f}\")\n",
        "print(f\"  ‚óè Balanced Accuracy: {test_bal_acc:.4f}\")\n",
        "\n",
        "# 3. V·∫Ω v√† l∆∞u Confusion Matrix\n",
        "classes = [idx2label[i] for i in range(len(idx2label))]\n",
        "test_cm_path = os.path.join(DIR_FIGURES, 'final_test_confusion_matrix.png')\n",
        "\n",
        "plot_final_cm(\n",
        "    all_labels, all_preds, classes,\n",
        "    save_path=test_cm_path,\n",
        "    title=f'Confusion Matrix - EfficientNet-B1 Fine-tuned\\nISIC 2018 Test Set'\n",
        ")\n",
        "\n",
        "# 4. In b√°o c√°o chi ti·∫øt ƒë·ªÉ ƒë∆∞a v√†o ph·ª• l·ª•c\n",
        "print(\"\\nüìã PH√ÇN T√çCH CHI TI·∫æT T·ª™NG L·ªöP (Classification Report):\")\n",
        "print(classification_report(all_labels, all_preds, target_names=classes))"
      ],
      "metadata": {
        "id": "REimm_gn4ONd"
      },
      "id": "REimm_gn4ONd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}