{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ISIC 2018 Skin Lesion Classification - EfficientNet B1\n",
                "\n",
                "**Environment:** GitHub Codespaces (Linux)  \n",
                "**Dataset:** ISIC 2018 Task 3 (7 classes)  \n",
                "**Model:** EfficientNet-B1 with pretrained ImageNet weights\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 1: Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Setup Environment\n",
                "\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Set working directory (Codespaces)\n",
                "REPO_ROOT = \"/workspaces/ISIC_2018\"\n",
                "if os.path.exists(REPO_ROOT):\n",
                "    os.chdir(REPO_ROOT)\n",
                "    print(f\"✓ Working directory: {os.getcwd()}\")\n",
                "else:\n",
                "    print(f\"⚠ Codespaces root not found, using current directory: {os.getcwd()}\")\n",
                "    REPO_ROOT = os.getcwd()\n",
                "\n",
                "# Add to Python path\n",
                "if REPO_ROOT not in sys.path:\n",
                "    sys.path.insert(0, REPO_ROOT)\n",
                "print(f\"✓ Python path updated\")\n",
                "\n",
                "# Check src folder\n",
                "if os.path.exists('src'):\n",
                "    print(\"✓ 'src' folder found!\")\n",
                "    print(\"  Files:\", os.listdir('src'))\n",
                "else:\n",
                "    print(\"❌ 'src' folder NOT found!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "%pip install -r requirements.txt -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check PyTorch and CUDA\n",
                "import torch\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
                "    !nvidia-smi\n",
                "else:\n",
                "    print(\"⚠ Running on CPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 2: Unzip Dataset (Idempotent)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "set -e\n",
                "\n",
                "cd /workspaces/ISIC_2018\n",
                "\n",
                "# Create data directory\n",
                "mkdir -p data/ISIC2018\n",
                "\n",
                "# Marker file to check if already unzipped\n",
                "MARKER=\"data/ISIC2018/.unzip_complete\"\n",
                "\n",
                "if [ -f \"$MARKER\" ]; then\n",
                "    echo \"✓ Dataset already unzipped (marker file exists)\"\n",
                "    echo \"  Delete $MARKER to re-extract\"\n",
                "else\n",
                "    echo \"Extracting dataset...\"\n",
                "    \n",
                "    # Unzip each file (-n = no overwrite, -q = quiet)\n",
                "    for zip in Training_Input.zip Validation_Input.zip Test_Input.zip \\\n",
                "               Training_GroundTruth.zip Validation_GroundTruth.zip Test_GroundTruth.zip; do\n",
                "        if [ -f \"$zip\" ]; then\n",
                "            echo \"  Extracting $zip...\"\n",
                "            unzip -n -q \"$zip\" -d data/ISIC2018/\n",
                "        else\n",
                "            echo \"  ⚠ $zip not found, skipping\"\n",
                "        fi\n",
                "    done\n",
                "    \n",
                "    echo \"\"\n",
                "    echo \"Normalizing folder names...\"\n",
                "    cd data/ISIC2018\n",
                "    \n",
                "    # Rename ISIC2018_Task3_* folders to simpler names if they exist\n",
                "    for dir in ISIC2018_Task3_Training_Input ISIC2018_Task3_Training Input; do\n",
                "        [ -d \"$dir\" ] && [ ! -d \"Training_Input\" ] && mv \"$dir\" Training_Input && echo \"  Renamed $dir -> Training_Input\"\n",
                "    done\n",
                "    \n",
                "    for dir in ISIC2018_Task3_Validation_Input ISIC2018_Task3_Validation Input; do\n",
                "        [ -d \"$dir\" ] && [ ! -d \"Validation_Input\" ] && mv \"$dir\" Validation_Input && echo \"  Renamed $dir -> Validation_Input\"\n",
                "    done\n",
                "    \n",
                "    for dir in ISIC2018_Task3_Test_Input ISIC2018_Task3_Test Input; do\n",
                "        [ -d \"$dir\" ] && [ ! -d \"Test_Input\" ] && mv \"$dir\" Test_Input && echo \"  Renamed $dir -> Test_Input\"\n",
                "    done\n",
                "    \n",
                "    for dir in ISIC2018_Task3_Training_GroundTruth; do\n",
                "        [ -d \"$dir\" ] && [ ! -d \"Training_GroundTruth\" ] && mv \"$dir\" Training_GroundTruth && echo \"  Renamed $dir -> Training_GroundTruth\"\n",
                "    done\n",
                "    \n",
                "    for dir in ISIC2018_Task3_Validation_GroundTruth; do\n",
                "        [ -d \"$dir\" ] && [ ! -d \"Validation_GroundTruth\" ] && mv \"$dir\" Validation_GroundTruth && echo \"  Renamed $dir -> Validation_GroundTruth\"\n",
                "    done\n",
                "    \n",
                "    for dir in ISIC2018_Task3_Test_GroundTruth; do\n",
                "        [ -d \"$dir\" ] && [ ! -d \"Test_GroundTruth\" ] && mv \"$dir\" Test_GroundTruth && echo \"  Renamed $dir -> Test_GroundTruth\"\n",
                "    done\n",
                "    \n",
                "    # Create marker file\n",
                "    cd /workspaces/ISIC_2018\n",
                "    touch \"$MARKER\"\n",
                "    echo \"\"\n",
                "    echo \"✓ Extraction complete!\"\n",
                "fi\n",
                "\n",
                "echo \"\"\n",
                "echo \"Directory structure:\"\n",
                "ls -la data/ISIC2018/ 2>/dev/null || echo \"  (empty)\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 3: Verify Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Verify Dataset\n",
                "\n",
                "from pathlib import Path\n",
                "\n",
                "DATA_ROOT = Path(\"/workspaces/ISIC_2018/data/ISIC2018\")\n",
                "\n",
                "# Check directories\n",
                "required_dirs = [\n",
                "    \"Training_Input\", \"Validation_Input\", \"Test_Input\",\n",
                "    \"Training_GroundTruth\", \"Validation_GroundTruth\", \"Test_GroundTruth\"\n",
                "]\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"DATASET VERIFICATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "all_found = True\n",
                "for d in required_dirs:\n",
                "    path = DATA_ROOT / d\n",
                "    if path.exists():\n",
                "        if \"Input\" in d:\n",
                "            count = len(list(path.glob(\"*.jpg\")))\n",
                "            print(f\"✓ {d}: {count} images\")\n",
                "        else:\n",
                "            csvs = list(path.glob(\"*.csv\"))\n",
                "            print(f\"✓ {d}: {len(csvs)} CSV(s) - {[c.name for c in csvs]}\")\n",
                "    else:\n",
                "        print(f\"❌ {d}: NOT FOUND\")\n",
                "        all_found = False\n",
                "\n",
                "print(\"=\" * 60)\n",
                "\n",
                "if all_found:\n",
                "    print(\"✓ All dataset files found!\")\n",
                "else:\n",
                "    print(\"⚠ Some files missing. Check unzip step.\")\n",
                "\n",
                "# Show sample paths\n",
                "print(\"\\nSample image paths:\")\n",
                "for d in [\"Training_Input\", \"Validation_Input\", \"Test_Input\"]:\n",
                "    path = DATA_ROOT / d\n",
                "    if path.exists():\n",
                "        samples = list(path.glob(\"*.jpg\"))[:2]\n",
                "        for s in samples:\n",
                "            print(f\"  {s}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 4: Import Modules"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Import Modules\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import torch\n",
                "from tqdm.notebook import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Import custom modules from src folder\n",
                "from src.config import *\n",
                "from src.data_processing import load_all_data\n",
                "from src.dataset import ISICDataset\n",
                "from src.transforms import get_train_transform, get_val_transform\n",
                "from src.model import build_model, count_parameters, load_checkpoint, save_checkpoint, print_model_info\n",
                "from src.train import train_one_epoch, create_dataloaders, get_optimizer, get_scheduler, get_criterion\n",
                "from src.evaluate import evaluate, plot_confusion_matrix, print_classification_report, create_submission, visualize_predictions\n",
                "\n",
                "# Set random seed\n",
                "set_seed(SEED)\n",
                "\n",
                "# Print configuration\n",
                "print_config()\n",
                "\n",
                "print(f\"\\n✓ All modules imported successfully!\")\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 5: Load and Prepare Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Load and Prepare Data\n",
                "\n",
                "# Load all data\n",
                "df_train, df_val, df_test, label2idx, idx2label, num_classes, use_weighted_sampler = load_all_data(\n",
                "    PATH_TRAIN_CSV, PATH_VAL_CSV, PATH_TEST_CSV,\n",
                "    DIR_TRAIN_IMG, DIR_VAL_IMG, DIR_TEST_IMG\n",
                ")\n",
                "\n",
                "# Store label mappings in config (for checkpoint saving)\n",
                "LABEL2IDX = label2idx\n",
                "IDX2LABEL = idx2label\n",
                "\n",
                "# Create datasets\n",
                "train_transform = get_train_transform()\n",
                "val_transform = get_val_transform()\n",
                "\n",
                "train_dataset = ISICDataset(df_train, transform=train_transform)\n",
                "val_dataset = ISICDataset(df_val, transform=val_transform)\n",
                "test_dataset = ISICDataset(df_test, transform=val_transform)\n",
                "\n",
                "print(f\"\\n✓ Datasets created:\")\n",
                "print(f\"  Train: {len(train_dataset)} samples\")\n",
                "print(f\"  Val: {len(val_dataset)} samples\")\n",
                "print(f\"  Test: {len(test_dataset)} samples\")\n",
                "\n",
                "# Create dataloaders\n",
                "train_loader, val_loader, test_loader = create_dataloaders(\n",
                "    df_train, df_val, df_test,\n",
                "    train_dataset, val_dataset, test_dataset,\n",
                "    BATCH_SIZE, NUM_WORKERS, use_weighted_sampler\n",
                ")\n",
                "\n",
                "print(f\"\\n✓ Dataloaders created:\")\n",
                "print(f\"  Train: {len(train_loader)} batches\")\n",
                "print(f\"  Val: {len(val_loader)} batches\")\n",
                "print(f\"  Test: {len(test_loader)} batches\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 6: Build Model and Training Components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Build Model and Training Components\n",
                "\n",
                "# Build model\n",
                "model = build_model(num_classes=num_classes, pretrained=True, model_name=MODEL_NAME)\n",
                "model = model.to(DEVICE)\n",
                "\n",
                "# Print model info\n",
                "print_model_info(model, MODEL_NAME.upper())\n",
                "\n",
                "# Setup training components\n",
                "optimizer = get_optimizer(model, LEARNING_RATE, WEIGHT_DECAY)\n",
                "scheduler = get_scheduler(optimizer, NUM_EPOCHS, USE_COSINE_SCHEDULER)\n",
                "criterion = get_criterion(USE_LABEL_SMOOTHING, LABEL_SMOOTHING)\n",
                "\n",
                "# Initialize mixed precision scaler\n",
                "from torch.cuda.amp import GradScaler\n",
                "scaler = GradScaler() if torch.cuda.is_available() else None\n",
                "\n",
                "print(\"\\n✓ Model and training components initialized!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 7: Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Training\n",
                "\n",
                "from src.engine import train_model\n",
                "import src.config as config  # NOTE: Correct import (not scr.config)\n",
                "\n",
                "# 1. Initialize history and tracking\n",
                "history = {\n",
                "    'epoch': [], 'train_loss': [], 'train_acc': [],\n",
                "    'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_bal_acc': [], 'lr': []\n",
                "}\n",
                "best_val_f1 = 0.0\n",
                "best_epoch = 0\n",
                "start_epoch = 0\n",
                "\n",
                "# 2. Load existing checkpoint if it exists\n",
                "if os.path.exists(MODEL_PATH):\n",
                "    checkpoint = load_checkpoint(model, optimizer, MODEL_PATH, DEVICE)\n",
                "    start_epoch = checkpoint.get('epoch', 0)\n",
                "    best_val_f1 = checkpoint.get('best_val_f1', 0.0)\n",
                "    best_epoch = checkpoint.get('best_epoch', 0)\n",
                "    if 'history' in checkpoint:\n",
                "        history = checkpoint['history']\n",
                "    print(f\"✓ Resumed from epoch {start_epoch}\")\n",
                "else:\n",
                "    print(\"✓ Starting fresh training\")\n",
                "\n",
                "# 3. Prepare config for engine\n",
                "cfg_dict = {\n",
                "    'VAL_EVERY_N_EPOCHS': config.VAL_EVERY_N_EPOCHS,\n",
                "    'SAVE_EVERY_N_EPOCHS': config.SAVE_EVERY_N_EPOCHS,\n",
                "    'EARLY_STOP_PATIENCE': config.EARLY_STOP_PATIENCE,\n",
                "    'USE_COSINE_SCHEDULER': config.USE_COSINE_SCHEDULER,\n",
                "    'MODEL_PATH': config.MODEL_PATH,\n",
                "    'NUM_CLASSES': config.NUM_CLASSES,\n",
                "    'label2idx': label2idx,\n",
                "    'idx2label': idx2label\n",
                "}\n",
                "\n",
                "# 4. Run Training\n",
                "model, history, best_val_f1, best_epoch = train_model(\n",
                "    model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
                "    num_epochs=config.NUM_EPOCHS,\n",
                "    device=DEVICE,\n",
                "    config_dict=cfg_dict,\n",
                "    start_epoch=start_epoch,\n",
                "    best_val_f1=best_val_f1,\n",
                "    best_epoch=best_epoch,\n",
                "    history=history\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 8: Evaluate on Validation Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Evaluate on Validation Set\n",
                "\n",
                "# Load best model\n",
                "print(\"Loading best model...\")\n",
                "checkpoint = load_checkpoint(model, None, MODEL_PATH, DEVICE)\n",
                "\n",
                "# Evaluate on validation set\n",
                "print(\"\\nEvaluating on validation set...\")\n",
                "val_loss, val_acc, val_f1, val_bal_acc, val_preds, val_labels, val_probs, val_image_ids = evaluate(\n",
                "    model, val_loader, criterion, DEVICE\n",
                ")\n",
                "\n",
                "print(f\"\\nValidation Results:\")\n",
                "print(f\"  Loss: {val_loss:.4f}\")\n",
                "print(f\"  Accuracy: {val_acc:.4f}\")\n",
                "print(f\"  Macro F1: {val_f1:.4f}\")\n",
                "print(f\"  Balanced Accuracy: {val_bal_acc:.4f}\")\n",
                "\n",
                "# Confusion Matrix\n",
                "plot_confusion_matrix(\n",
                "    val_labels, val_preds, idx2label,\n",
                "    save_path=os.path.join(DIR_FIGURES, 'val_confusion_matrix.png'),\n",
                "    title='Validation Confusion Matrix'\n",
                ")\n",
                "\n",
                "# Classification Report\n",
                "print_classification_report(val_labels, val_preds, idx2label)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 9: Evaluate on Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 9: Evaluate on Test Set\n",
                "\n",
                "# Evaluate on test set\n",
                "print(\"Evaluating on test set...\")\n",
                "test_loss, test_acc, test_f1, test_bal_acc, test_preds, test_labels, test_probs, test_image_ids = evaluate(\n",
                "    model, test_loader, criterion, DEVICE\n",
                ")\n",
                "\n",
                "print(f\"\\nTest Results:\")\n",
                "print(f\"  Loss: {test_loss:.4f}\")\n",
                "print(f\"  Accuracy: {test_acc:.4f}\")\n",
                "print(f\"  Macro F1: {test_f1:.4f}\")\n",
                "print(f\"  Balanced Accuracy: {test_bal_acc:.4f}\")\n",
                "\n",
                "# Confusion Matrix\n",
                "plot_confusion_matrix(\n",
                "    test_labels, test_preds, idx2label,\n",
                "    save_path=os.path.join(DIR_FIGURES, 'test_confusion_matrix.png'),\n",
                "    title='Test Confusion Matrix'\n",
                ")\n",
                "\n",
                "# Classification Report\n",
                "print_classification_report(test_labels, test_preds, idx2label)\n",
                "\n",
                "# Create submission\n",
                "submission_path = os.path.join(DIR_SUBMISSIONS, 'test_predictions.csv')\n",
                "submission_df = create_submission(test_image_ids, test_preds, test_probs, idx2label, submission_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 10: Inference Demo (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 10: Inference Demo (Optional)\n",
                "\n",
                "# Demo images - check if they exist first\n",
                "demo_paths = [\n",
                "    os.path.join(DIR_TEST_IMG, \"ISIC_0034524.jpg\"),\n",
                "    os.path.join(DIR_VAL_IMG, \"ISIC_0034321.jpg\"),\n",
                "]\n",
                "\n",
                "demo_images = []\n",
                "for path in demo_paths:\n",
                "    if os.path.exists(path):\n",
                "        img_id = os.path.basename(path).replace(\".jpg\", \"\")\n",
                "        demo_images.append((path, img_id))\n",
                "\n",
                "if demo_images:\n",
                "    print(\"\\n=== INFERENCE DEMO ===\")\n",
                "    \n",
                "    # Visualize predictions\n",
                "    visualize_predictions(\n",
                "        model, demo_images, val_transform, DEVICE, idx2label,\n",
                "        save_path=os.path.join(DIR_FIGURES, 'inference_demo.png'),\n",
                "        top_k=3\n",
                "    )\n",
                "    \n",
                "    print(\"\\n✓ Inference demo completed!\")\n",
                "else:\n",
                "    print(\"\\n⚠ Demo images not found, skipping inference demo.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cell 11: Training History Visualization (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 11: Training History Visualization (Optional)\n",
                "\n",
                "if history and len(history.get('epoch', [])) > 0:\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "    \n",
                "    # Loss\n",
                "    axes[0].plot(history['epoch'], history['train_loss'], label='Train')\n",
                "    axes[0].plot(history['epoch'], history['val_loss'], label='Val')\n",
                "    axes[0].set_xlabel('Epoch')\n",
                "    axes[0].set_ylabel('Loss')\n",
                "    axes[0].set_title('Training & Validation Loss')\n",
                "    axes[0].legend()\n",
                "    axes[0].grid(True)\n",
                "    \n",
                "    # Accuracy\n",
                "    axes[1].plot(history['epoch'], history['train_acc'], label='Train')\n",
                "    axes[1].plot(history['epoch'], history['val_acc'], label='Val')\n",
                "    axes[1].set_xlabel('Epoch')\n",
                "    axes[1].set_ylabel('Accuracy')\n",
                "    axes[1].set_title('Training & Validation Accuracy')\n",
                "    axes[1].legend()\n",
                "    axes[1].grid(True)\n",
                "    \n",
                "    # F1 Score\n",
                "    axes[2].plot(history['epoch'], history['val_f1'], label='Val F1', color='green')\n",
                "    if best_epoch > 0:\n",
                "        axes[2].axvline(x=best_epoch, color='r', linestyle='--', label=f'Best (Epoch {best_epoch})')\n",
                "    axes[2].set_xlabel('Epoch')\n",
                "    axes[2].set_ylabel('F1 Score')\n",
                "    axes[2].set_title('Validation F1 Score')\n",
                "    axes[2].legend()\n",
                "    axes[2].grid(True)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(os.path.join(DIR_FIGURES, 'training_history.png'), dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"\\n✓ Training history saved: {os.path.join(DIR_FIGURES, 'training_history.png')}\")\n",
                "else:\n",
                "    print(\"\\n⚠ No training history available to plot.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "This notebook:\n",
                "1. ✓ Sets up the environment for GitHub Codespaces\n",
                "2. ✓ Extracts dataset from zip files (idempotent)\n",
                "3. ✓ Verifies dataset structure\n",
                "4. ✓ Loads data and creates dataloaders\n",
                "5. ✓ Builds EfficientNet-B1 model\n",
                "6. ✓ Trains with early stopping and checkpointing\n",
                "7. ✓ Evaluates on validation and test sets\n",
                "8. ✓ Creates submission file"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}